from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time

def scrape_ncaa_player_positions():
    url = "https://www.espn.com/mens-college-basketball/stats/player"

    # Configure Chrome options (window opens visibly)
    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")  # Open in full screen

    # Initialize WebDriver
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)

    # Wait for the stats table to load
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".Table__TBODY tr"))
        )
        print("üìä NCAA player stats table loaded.")
    except Exception as e:
        print(f"‚ö†Ô∏è ERROR: Stats table not found. {e}")
        driver.quit()
        return

    # Click "Show More" button until all players are loaded
    while True:
        try:
            show_more_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "div.tc.mv5.loadMore a.loadMore__link"))
            )
            driver.execute_script("arguments[0].click();", show_more_button)
            print("üîÑ Clicked 'Show More' button.")
            time.sleep(2)  # Allow time for new players to load
        except:
            print("‚úÖ No more 'Show More' button found. All players loaded.")
            break  # Exit loop when no more button is found

    # Extract player positions
    positions = []
    rows = driver.find_elements(By.CSS_SELECTOR, ".Table__TBODY tr")

    for row in rows:
        try:
            cols = row.find_elements(By.TAG_NAME, "td")

            # Extract Position
            position = cols[1].find_element(By.TAG_NAME, "span").text.strip() if len(cols[1].find_elements(By.TAG_NAME, "span")) > 0 else "N/A"

            positions.append([position])

        except Exception as e:
            print(f"‚ö†Ô∏è Skipping row due to error: {e}")
            continue

    # Close the browser
    driver.quit()

    # Create DataFrame
    df = pd.DataFrame(positions, columns=["Position"])

    # Save to CSV
    df.to_csv("ncaa_player_positions.csv", index=False)
    print("‚úÖ Data extraction complete! File saved as 'ncaa_player_positions.csv'.")

# Run the scraper
scrape_ncaa_player_positions()
